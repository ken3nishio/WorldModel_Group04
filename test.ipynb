{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b9e873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'WorldModel_Group04'...\n",
      "remote: Enumerating objects: 163, done.\u001b[K\n",
      "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
      "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
      "remote: Total 163 (delta 66), reused 157 (delta 61), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (163/163), 1.00 MiB | 4.33 MiB/s, done.\n",
      "Resolving deltas: 100% (66/66), done.\n",
      "/content/WorldModel_Group04\n",
      "Requirement already satisfied: torch>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.24.0+cu126)\n",
      "Requirement already satisfied: accelerate>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (1.12.0)\n",
      "Requirement already satisfied: diffusers>=0.33.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (0.36.0)\n",
      "Requirement already satisfied: transformers>=4.46.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 5)) (5.0.0)\n",
      "Requirement already satisfied: gradio>=5.23.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 6)) (5.50.0)\n",
      "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 7)) (0.2.1)\n",
      "Requirement already satisfied: pillow>=11.1.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (11.3.0)\n",
      "Collecting av>=12.1.0 (from -r requirements.txt (line 9))\n",
      "  Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 10)) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 11)) (1.16.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 12)) (2.32.4)\n",
      "Collecting torchsde>=0.2.6 (from -r requirements.txt (line 13))\n",
      "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 14)) (0.8.2)\n",
      "Requirement already satisfied: opencv-python>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 15)) (4.13.0.90)\n",
      "Requirement already satisfied: safetensors>=0.4.5 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 16)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 17)) (4.67.2)\n",
      "Collecting pandas>=2.2.3 (from -r requirements.txt (line 18))\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 19)) (3.10.0)\n",
      "Collecting lpips>=0.1.4 (from -r requirements.txt (line 20))\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.2.0->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.6.0->-r requirements.txt (line 3)) (26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.6.0->-r requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.6.0->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=1.6.0->-r requirements.txt (line 3)) (1.3.7)\n",
      "Requirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.33.1->-r requirements.txt (line 4)) (8.7.1)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.33.1->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from diffusers>=0.33.1->-r requirements.txt (line 4)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.2->-r requirements.txt (line 5)) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers>=4.46.2->-r requirements.txt (line 5)) (0.21.1)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (4.12.1)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.123.10)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (3.0.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (3.11.7)\n",
      "Collecting pandas>=2.2.3 (from -r requirements.txt (line 18))\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (2.12.3)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.0.22)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.14.14)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.21.1)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=5.23.0->-r requirements.txt (line 6)) (0.40.0)\n",
      "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio>=5.23.0->-r requirements.txt (line 6)) (15.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 12)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 12)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->-r requirements.txt (line 12)) (2026.1.4)\n",
      "Collecting trampoline>=0.1.2 (from torchsde>=0.2.6->-r requirements.txt (line 13))\n",
      "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->-r requirements.txt (line 18)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->-r requirements.txt (line 18)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.2.3->-r requirements.txt (line 18)) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 19)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 19)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 19)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 19)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.8.0->-r requirements.txt (line 19)) (3.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio>=5.23.0->-r requirements.txt (line 6)) (0.0.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->diffusers>=0.33.1->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->diffusers>=0.33.1->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.6.0->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.6.0->-r requirements.txt (line 3)) (1.5.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=5.23.0->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=5.23.0->-r requirements.txt (line 6)) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio>=5.23.0->-r requirements.txt (line 6)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->-r requirements.txt (line 18)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.2.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=5.23.0->-r requirements.txt (line 6)) (8.3.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=5.23.0->-r requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers>=0.33.1->-r requirements.txt (line 4)) (3.23.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.23.0->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.23.0->-r requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=5.23.0->-r requirements.txt (line 6)) (0.1.2)\n",
      "Downloading av-16.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (41.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: trampoline, av, pandas, torchsde, lpips\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed av-16.1.0 lpips-0.1.4 pandas-2.3.3 torchsde-0.2.6 trampoline-0.1.2\n",
      "Collecting git+https://github.com/Vchitect/VBench.git\n",
      "  Cloning https://github.com/Vchitect/VBench.git to /tmp/pip-req-build-2uwqf429\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/Vchitect/VBench.git /tmp/pip-req-build-2uwqf429\n",
      "  Resolved https://github.com/Vchitect/VBench.git to commit 76a4b7fe281bff69ce2703e212a864be4ebb2d09\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!git clone -b experiment https://github.com/ken3nishio/WorldModel_Group04.git\n",
    "%cd WorldModel_Group04\n",
    "\n",
    "# uvを使わず、標準のpipを使用する（Colabのランタイム環境に直接インストール）\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# VBenchが必要なら追加\n",
    "!pip install git+https://github.com/Vchitect/VBench.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "008e1d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch                                    2.9.0+cu126\n",
      "torchao                                  0.10.0\n",
      "torchaudio                               2.9.0+cu126\n",
      "torchcodec                               0.8.0+cu126\n",
      "torchdata                                0.11.0\n",
      "torchsde                                 0.2.6\n",
      "torchsummary                             1.5.1\n",
      "torchtune                                0.6.1\n",
      "torchvision                              0.24.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# インストール確認\n",
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7056ef11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import json\n",
      "import torch\n",
      "import numpy as np\n",
      "import argparse\n",
      "import time\n",
      "from PIL import Image\n",
      "from transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer, SiglipImageProcessor, SiglipVisionModel\n",
      "from diffusers import AutoencoderKLHunyuanVideo\n",
      "from diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake\n",
      "from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, generate_timestamp\n",
      "from diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked\n",
      "from diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan\n",
      "from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, DynamicSwapInstaller, unload_complete_models, load_model_as_complete, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation\n",
      "from diffusers_helper.clip_vision import hf_clip_vision_encode\n",
      "from diffusers_helper.bucket_tools import find_nearest_bucket\n",
      "import einops\n",
      "\n",
      "# Add evaluation path\n",
      "import sys\n",
      "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
      "from evaluation.run_vbench_custom import run_evaluation\n",
      "\n",
      "class BenchmarkRunner:\n",
      "    def __init__(self, output_base_dir=\"experiments/results\"):\n",
      "        self.output_base_dir = output_base_dir\n",
      "        self.timestamp = generate_timestamp()\n",
      "        self.output_dir = os.path.join(self.output_base_dir, self.timestamp)\n",
      "        os.makedirs(self.output_dir, exist_ok=True)\n",
      "        \n",
      "        # Load Models\n",
      "        print(\"Loading models...\")\n",
      "        self.text_encoder = LlamaModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder', torch_dtype=torch.float16).cpu()\n",
      "        self.text_encoder_2 = CLIPTextModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder_2', torch_dtype=torch.float16).cpu()\n",
      "        self.tokenizer = LlamaTokenizerFast.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer')\n",
      "        self.tokenizer_2 = CLIPTokenizer.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer_2')\n",
      "        self.vae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='vae', torch_dtype=torch.float16).cpu()\n",
      "        \n",
      "        self.feature_extractor = SiglipImageProcessor.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='feature_extractor')\n",
      "        self.image_encoder = SiglipVisionModel.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='image_encoder', torch_dtype=torch.float16).cpu()\n",
      "        \n",
      "        self.transformer = HunyuanVideoTransformer3DModelPacked.from_pretrained('lllyasviel/FramePack_F1_I2V_HY_20250503', torch_dtype=torch.bfloat16).cpu()\n",
      "        \n",
      "        # Set Eval Mode\n",
      "        self.vae.eval()\n",
      "        self.text_encoder.eval()\n",
      "        self.text_encoder_2.eval()\n",
      "        self.image_encoder.eval()\n",
      "        self.transformer.eval()\n",
      "        \n",
      "        # Optimization\n",
      "        self.free_mem_gb = get_cuda_free_memory_gb(gpu)\n",
      "        self.high_vram = self.free_mem_gb > 60\n",
      "        \n",
      "        if not self.high_vram:\n",
      "            self.vae.enable_slicing()\n",
      "            self.vae.enable_tiling()\n",
      "            \n",
      "        self.transformer.high_quality_fp32_output_for_inference = True\n",
      "        \n",
      "        # Move relevant models to dtype\n",
      "        self.transformer.to(dtype=torch.bfloat16)\n",
      "        self.vae.to(dtype=torch.float16)\n",
      "        self.image_encoder.to(dtype=torch.float16)\n",
      "        self.text_encoder.to(dtype=torch.float16)\n",
      "        self.text_encoder_2.to(dtype=torch.float16)\n",
      "        \n",
      "        # No grad\n",
      "        self.vae.requires_grad_(False)\n",
      "        self.text_encoder.requires_grad_(False)\n",
      "        self.text_encoder_2.requires_grad_(False)\n",
      "        self.image_encoder.requires_grad_(False)\n",
      "        self.transformer.requires_grad_(False)\n",
      "        \n",
      "        if not self.high_vram:\n",
      "             DynamicSwapInstaller.install_model(self.transformer, device=gpu)\n",
      "        else:\n",
      "             self.text_encoder.to(gpu)\n",
      "             self.text_encoder_2.to(gpu)\n",
      "             self.image_encoder.to(gpu)\n",
      "             self.vae.to(gpu)\n",
      "             self.transformer.to(gpu)\n",
      "             \n",
      "        print(\"Models loaded successfully.\")\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(self, prompt, input_image, seed, adaptive_cfg_beta, steps=25, cfg=1.0, gs=10.0, rs=0.0):\n",
      "        print(f\"Generating for prompt: '{prompt}' with beta={adaptive_cfg_beta}, seed={seed}\")\n",
      "        \n",
      "        # Clean GPU\n",
      "        if not self.high_vram:\n",
      "            unload_complete_models(self.text_encoder, self.text_encoder_2, self.image_encoder, self.vae, self.transformer)\n",
      "\n",
      "        # Text encoding\n",
      "        if not self.high_vram:\n",
      "            load_model_as_complete(self.text_encoder, target_device=gpu)\n",
      "            load_model_as_complete(self.text_encoder_2, target_device=gpu, unload=False)\n",
      "\n",
      "        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, self.text_encoder, self.text_encoder_2, self.tokenizer, self.tokenizer_2)\n",
      "        \n",
      "        # cfg=1.0 usually implies no negative prompt needed for CFG calculation in standard pipelines, \n",
      "        # but Hunyuan logic often computes empty uncond embedding\n",
      "        if cfg == 1:\n",
      "             llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)\n",
      "        else:\n",
      "             # Empty negative prompt\n",
      "             llama_vec_n, clip_l_pooler_n = encode_prompt_conds(\"\", self.text_encoder, self.text_encoder_2, self.tokenizer, self.tokenizer_2)\n",
      "\n",
      "        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n",
      "        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)\n",
      "\n",
      "        # Image encoding\n",
      "        H, W, C = input_image.shape\n",
      "        height, width = find_nearest_bucket(H, W, resolution=640)\n",
      "        input_image_np = resize_and_center_crop(input_image, target_width=width, target_height=height)\n",
      "        \n",
      "        input_image_pt = torch.from_numpy(input_image_np).float() / 127.5 - 1\n",
      "        input_image_pt = input_image_pt.permute(2, 0, 1)[None, :, None]\n",
      "\n",
      "        # VAE Encode\n",
      "        if not self.high_vram:\n",
      "            load_model_as_complete(self.vae, target_device=gpu)\n",
      "            \n",
      "        start_latent = vae_encode(input_image_pt, self.vae)\n",
      "\n",
      "        # CLIP Vision\n",
      "        if not self.high_vram:\n",
      "            load_model_as_complete(self.image_encoder, target_device=gpu)\n",
      "            \n",
      "        image_encoder_output = hf_clip_vision_encode(input_image_np, self.feature_extractor, self.image_encoder)\n",
      "        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state\n",
      "\n",
      "        # Cast\n",
      "        llama_vec = llama_vec.to(self.transformer.dtype)\n",
      "        llama_vec_n = llama_vec_n.to(self.transformer.dtype)\n",
      "        clip_l_pooler = clip_l_pooler.to(self.transformer.dtype)\n",
      "        clip_l_pooler_n = clip_l_pooler_n.to(self.transformer.dtype)\n",
      "        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(self.transformer.dtype)\n",
      "\n",
      "        # Generating Loop Setup\n",
      "        rnd = torch.Generator(\"cpu\").manual_seed(seed)\n",
      "        \n",
      "        # Simple configuration for benchmark: fixed latent window size 9, total 2 sections (approx 2-3 sec)\n",
      "        latent_window_size = 9\n",
      "        total_latent_sections = 1 # Keep it short for benchmark (approx 1.2s) - change to 2 for longer\n",
      "        \n",
      "        history_latents = torch.zeros(size=(1, 16, 16 + 2 + 1, height // 8, width // 8), dtype=torch.float32).cpu()\n",
      "        history_latents = torch.cat([history_latents, start_latent.to(history_latents)], dim=2)\n",
      "        history_pixels = None\n",
      "        \n",
      "        total_generated_latent_frames = 1\n",
      "\n",
      "        for section_index in range(total_latent_sections):\n",
      "            if not self.high_vram:\n",
      "                unload_complete_models()\n",
      "                move_model_to_device_with_memory_preservation(self.transformer, target_device=gpu, preserved_memory_gb=6.0)\n",
      "                \n",
      "            # Teacache disabled for benchmark accuracy\n",
      "            self.transformer.initialize_teacache(enable_teacache=False)\n",
      "            \n",
      "            indices = torch.arange(0, sum([1, 16, 2, 1, latent_window_size])).unsqueeze(0)\n",
      "            clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split([1, 16, 2, 1, latent_window_size], dim=1)\n",
      "            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)\n",
      "\n",
      "            clean_latents_4x, clean_latents_2x, clean_latents_1x = history_latents[:, :, -sum([16, 2, 1]):, :, :].split([16, 2, 1], dim=2)\n",
      "            clean_latents = torch.cat([start_latent.to(history_latents), clean_latents_1x], dim=2)\n",
      "\n",
      "            generated_latents = sample_hunyuan(\n",
      "                transformer=self.transformer,\n",
      "                sampler='unipc',\n",
      "                width=width,\n",
      "                height=height,\n",
      "                frames=latent_window_size * 4 - 3,\n",
      "                real_guidance_scale=cfg,\n",
      "                distilled_guidance_scale=gs,\n",
      "                guidance_rescale=rs,\n",
      "                num_inference_steps=steps,\n",
      "                generator=rnd,\n",
      "                prompt_embeds=llama_vec,\n",
      "                prompt_embeds_mask=llama_attention_mask,\n",
      "                prompt_poolers=clip_l_pooler,\n",
      "                negative_prompt_embeds=llama_vec_n,\n",
      "                negative_prompt_embeds_mask=llama_attention_mask_n,\n",
      "                negative_prompt_poolers=clip_l_pooler_n,\n",
      "                device=gpu,\n",
      "                dtype=torch.bfloat16,\n",
      "                image_embeddings=image_encoder_last_hidden_state,\n",
      "                latent_indices=latent_indices,\n",
      "                clean_latents=clean_latents,\n",
      "                clean_latent_indices=clean_latent_indices,\n",
      "                clean_latents_2x=clean_latents_2x,\n",
      "                clean_latent_2x_indices=clean_latent_2x_indices,\n",
      "                clean_latents_4x=clean_latents_4x,\n",
      "                clean_latent_4x_indices=clean_latent_4x_indices,\n",
      "                # Step-Adaptive CFG\n",
      "                adaptive_cfg_beta=adaptive_cfg_beta,\n",
      "                adaptive_cfg_min=1.0,\n",
      "            )\n",
      "\n",
      "            total_generated_latent_frames += int(generated_latents.shape[2])\n",
      "            history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)\n",
      "            \n",
      "            if not self.high_vram:\n",
      "                offload_model_from_device_for_memory_preservation(self.transformer, target_device=gpu, preserved_memory_gb=8)\n",
      "                load_model_as_complete(self.vae, target_device=gpu)\n",
      "\n",
      "            real_history_latents = history_latents[:, :, -total_generated_latent_frames:, :, :]\n",
      "            \n",
      "            if history_pixels is None:\n",
      "                history_pixels = vae_decode(real_history_latents, self.vae).cpu()\n",
      "            else:\n",
      "                 # Simplified stitching logic for benchmark purposes\n",
      "                section_latent_frames = latent_window_size * 2\n",
      "                overlapped_frames = latent_window_size * 4 - 3\n",
      "                current_pixels = vae_decode(real_history_latents[:, :, -section_latent_frames:], self.vae).cpu()\n",
      "                history_pixels = soft_append_bcthw(history_pixels, current_pixels, overlapped_frames)\n",
      "\n",
      "            if not self.high_vram:\n",
      "                unload_complete_models()\n",
      "                \n",
      "        return history_pixels\n",
      "\n",
      "def run_benchmark(prompts_file):\n",
      "    runner = BenchmarkRunner()\n",
      "    \n",
      "    # Check if inputs directory exists\n",
      "    inputs_dir = \"experiments/inputs\"\n",
      "    if not os.path.exists(inputs_dir):\n",
      "        print(f\"Warning: Inputs directory '{inputs_dir}' does not exist.\")\n",
      "    \n",
      "    with open(prompts_file, 'r') as f:\n",
      "        prompts = json.load(f)\n",
      "        \n",
      "    # Compare Baseline (beta=0.0) vs Proposed (beta=0.7)\n",
      "    betas = [0.0, 0.7]\n",
      "    \n",
      "    metadata_list = []\n",
      "    \n",
      "    for case in prompts:\n",
      "        prompt = case[\"prompt\"]\n",
      "        seed = case.get(\"seed\", 42)\n",
      "        category = case.get(\"category\", \"Unknown\")\n",
      "        case_id = case.get(\"id\", \"unk\")\n",
      "        \n",
      "        # Resolve Input Image Path\n",
      "        input_image_path = case.get(\"input_image\")\n",
      "        if not input_image_path or not os.path.exists(input_image_path):\n",
      "            print(f\"Error: Input image not found for case {case_id}: {input_image_path}\")\n",
      "            print(\"Skipping this case.\")\n",
      "            continue\n",
      "            \n",
      "        print(f\"Loading input image: {input_image_path}\")\n",
      "        start_image = np.array(Image.open(input_image_path).convert(\"RGB\"))\n",
      "        \n",
      "        for beta in betas:\n",
      "            print(f\"\\n--- Processing {case_id} (Category: {category}) with Beta={beta} ---\")\n",
      "            \n",
      "            history_pixels = runner.generate(\n",
      "                prompt=prompt,\n",
      "                input_image=start_image,\n",
      "                seed=seed,\n",
      "                adaptive_cfg_beta=beta\n",
      "            )\n",
      "            \n",
      "            filename = f\"{case_id}_beta{beta}_{runner.timestamp}.mp4\"\n",
      "            filepath = os.path.join(runner.output_dir, filename)\n",
      "            \n",
      "            save_bcthw_as_mp4(history_pixels, filepath, fps=30, crf=16)\n",
      "            print(f\"Saved to {filepath}\")\n",
      "            \n",
      "            metadata_list.append({\n",
      "                \"filename\": filename,\n",
      "                \"prompt\": prompt,\n",
      "                \"category\": category,\n",
      "                \"beta\": beta,\n",
      "                \"case_id\": case_id,\n",
      "                \"seed\": seed,\n",
      "                \"input_image\": input_image_path\n",
      "            })\n",
      "            \n",
      "    # Save Metadata\n",
      "    metadata_path = os.path.join(runner.output_dir, \"metadata.json\")\n",
      "    with open(metadata_path, 'w') as f:\n",
      "        json.dump(metadata_list, f, indent=2)\n",
      "    print(f\"Metadata saved to {metadata_path}\")\n",
      "    \n",
      "    # Run Evaluation\n",
      "    if len(metadata_list) > 0:\n",
      "        print(\"\\n--- Starting Evaluation ---\")\n",
      "        run_evaluation(\n",
      "            video_dir=runner.output_dir,\n",
      "            metadata_path=metadata_path,\n",
      "            output_dir=runner.output_dir, # Save eval results in the same folder\n",
      "            device='cuda'\n",
      "        )\n",
      "        print(\"\\n--- Benchmark Complete ---\")\n",
      "    else:\n",
      "        print(\"\\n--- No valid cases processed. Evaluation skipped. ---\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    run_benchmark(\"experiments/benchmark_prompts.json\")\n"
     ]
    }
   ],
   "source": [
    "!cat experiments/run_benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32da7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiments/run_benchmark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiments/run_benchmark.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path explicitly BEFORE importing diffusers_helper\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "from transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer, SiglipImageProcessor, SiglipVisionModel\n",
    "from diffusers import AutoencoderKLHunyuanVideo\n",
    "from diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode, vae_decode_fake\n",
    "from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, generate_timestamp\n",
    "from diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked\n",
    "from diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan\n",
    "from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, DynamicSwapInstaller, unload_complete_models, load_model_as_complete, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation\n",
    "from diffusers_helper.clip_vision import hf_clip_vision_encode\n",
    "from diffusers_helper.bucket_tools import find_nearest_bucket\n",
    "import einops\n",
    "\n",
    "# Evaluation import\n",
    "from evaluation.run_vbench_custom import run_evaluation\n",
    "\n",
    "class BenchmarkRunner:\n",
    "    def __init__(self, output_base_dir=\"experiments/results\"):\n",
    "        self.output_base_dir = output_base_dir\n",
    "        self.timestamp = generate_timestamp()\n",
    "        self.output_dir = os.path.join(self.output_base_dir, self.timestamp)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        # Load Models\n",
    "        print(\"Loading models...\")\n",
    "        self.text_encoder = LlamaModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder', torch_dtype=torch.float16).cpu()\n",
    "        self.text_encoder_2 = CLIPTextModel.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='text_encoder_2', torch_dtype=torch.float16).cpu()\n",
    "        self.tokenizer = LlamaTokenizerFast.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer')\n",
    "        self.tokenizer_2 = CLIPTokenizer.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='tokenizer_2')\n",
    "        self.vae = AutoencoderKLHunyuanVideo.from_pretrained(\"hunyuanvideo-community/HunyuanVideo\", subfolder='vae', torch_dtype=torch.float16).cpu()\n",
    "        \n",
    "        self.feature_extractor = SiglipImageProcessor.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='feature_extractor')\n",
    "        self.image_encoder = SiglipVisionModel.from_pretrained(\"lllyasviel/flux_redux_bfl\", subfolder='image_encoder', torch_dtype=torch.float16).cpu()\n",
    "        \n",
    "        self.transformer = HunyuanVideoTransformer3DModelPacked.from_pretrained('lllyasviel/FramePack_F1_I2V_HY_20250503', torch_dtype=torch.bfloat16).cpu()\n",
    "        \n",
    "        # Set Eval Mode\n",
    "        self.vae.eval()\n",
    "        self.text_encoder.eval()\n",
    "        self.text_encoder_2.eval()\n",
    "        self.image_encoder.eval()\n",
    "        self.transformer.eval()\n",
    "        \n",
    "        # Optimization\n",
    "        self.free_mem_gb = get_cuda_free_memory_gb(gpu)\n",
    "        self.high_vram = self.free_mem_gb > 60\n",
    "        \n",
    "        if not self.high_vram:\n",
    "            self.vae.enable_slicing()\n",
    "            self.vae.enable_tiling()\n",
    "            \n",
    "        self.transformer.high_quality_fp32_output_for_inference = True\n",
    "        \n",
    "        # Move relevant models to dtype\n",
    "        self.transformer.to(dtype=torch.bfloat16)\n",
    "        self.vae.to(dtype=torch.float16)\n",
    "        self.image_encoder.to(dtype=torch.float16)\n",
    "        self.text_encoder.to(dtype=torch.float16)\n",
    "        self.text_encoder_2.to(dtype=torch.float16)\n",
    "        \n",
    "        # No grad\n",
    "        self.vae.requires_grad_(False)\n",
    "        self.text_encoder.requires_grad_(False)\n",
    "        self.text_encoder_2.requires_grad_(False)\n",
    "        self.image_encoder.requires_grad_(False)\n",
    "        self.transformer.requires_grad_(False)\n",
    "        \n",
    "        if not self.high_vram:\n",
    "             DynamicSwapInstaller.install_model(self.transformer, device=gpu)\n",
    "        else:\n",
    "             self.text_encoder.to(gpu)\n",
    "             self.text_encoder_2.to(gpu)\n",
    "             self.image_encoder.to(gpu)\n",
    "             self.vae.to(gpu)\n",
    "             self.transformer.to(gpu)\n",
    "             \n",
    "        print(\"Models loaded successfully.\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt, input_image, seed, adaptive_cfg_beta, steps=25, cfg=1.0, gs=10.0, rs=0.0):\n",
    "        print(f\"Generating for prompt: '{prompt}' with beta={adaptive_cfg_beta}, seed={seed}\")\n",
    "        \n",
    "        # Clean GPU\n",
    "        if not self.high_vram:\n",
    "            unload_complete_models(self.text_encoder, self.text_encoder_2, self.image_encoder, self.vae, self.transformer)\n",
    "\n",
    "        # Text encoding\n",
    "        if not self.high_vram:\n",
    "            load_model_as_complete(self.text_encoder, target_device=gpu)\n",
    "            load_model_as_complete(self.text_encoder_2, target_device=gpu, unload=False)\n",
    "\n",
    "        llama_vec, clip_l_pooler = encode_prompt_conds(prompt, self.text_encoder, self.text_encoder_2, self.tokenizer, self.tokenizer_2)\n",
    "        \n",
    "        # cfg=1.0 usually implies no negative prompt needed for CFG calculation in standard pipelines, \n",
    "        # but Hunyuan logic often computes empty uncond embedding\n",
    "        if cfg == 1:\n",
    "             llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)\n",
    "        else:\n",
    "             # Empty negative prompt\n",
    "             llama_vec_n, clip_l_pooler_n = encode_prompt_conds(\"\", self.text_encoder, self.text_encoder_2, self.tokenizer, self.tokenizer_2)\n",
    "\n",
    "        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)\n",
    "        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)\n",
    "\n",
    "        # Image encoding\n",
    "        H, W, C = input_image.shape\n",
    "        height, width = find_nearest_bucket(H, W, resolution=640)\n",
    "        input_image_np = resize_and_center_crop(input_image, target_width=width, target_height=height)\n",
    "        \n",
    "        input_image_pt = torch.from_numpy(input_image_np).float() / 127.5 - 1\n",
    "        input_image_pt = input_image_pt.permute(2, 0, 1)[None, :, None]\n",
    "\n",
    "        # VAE Encode\n",
    "        if not self.high_vram:\n",
    "            load_model_as_complete(self.vae, target_device=gpu)\n",
    "            \n",
    "        start_latent = vae_encode(input_image_pt, self.vae)\n",
    "\n",
    "        # CLIP Vision\n",
    "        if not self.high_vram:\n",
    "            load_model_as_complete(self.image_encoder, target_device=gpu)\n",
    "            \n",
    "        image_encoder_output = hf_clip_vision_encode(input_image_np, self.feature_extractor, self.image_encoder)\n",
    "        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state\n",
    "\n",
    "        # Cast\n",
    "        llama_vec = llama_vec.to(self.transformer.dtype)\n",
    "        llama_vec_n = llama_vec_n.to(self.transformer.dtype)\n",
    "        clip_l_pooler = clip_l_pooler.to(self.transformer.dtype)\n",
    "        clip_l_pooler_n = clip_l_pooler_n.to(self.transformer.dtype)\n",
    "        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(self.transformer.dtype)\n",
    "\n",
    "        # Generating Loop Setup\n",
    "        rnd = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "        \n",
    "        # Simple configuration for benchmark: fixed latent window size 9, total 2 sections (approx 2-3 sec)\n",
    "        latent_window_size = 9\n",
    "        total_latent_sections = 1 # Keep it short for benchmark (approx 1.2s) - change to 2 for longer\n",
    "        \n",
    "        history_latents = torch.zeros(size=(1, 16, 16 + 2 + 1, height // 8, width // 8), dtype=torch.float32).cpu()\n",
    "        history_latents = torch.cat([history_latents, start_latent.to(history_latents)], dim=2)\n",
    "        history_pixels = None\n",
    "        \n",
    "        total_generated_latent_frames = 1\n",
    "\n",
    "        for section_index in range(total_latent_sections):\n",
    "            if not self.high_vram:\n",
    "                unload_complete_models()\n",
    "                move_model_to_device_with_memory_preservation(self.transformer, target_device=gpu, preserved_memory_gb=6.0)\n",
    "                \n",
    "            # Teacache disabled for benchmark accuracy\n",
    "            self.transformer.initialize_teacache(enable_teacache=False)\n",
    "            \n",
    "            indices = torch.arange(0, sum([1, 16, 2, 1, latent_window_size])).unsqueeze(0)\n",
    "            clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split([1, 16, 2, 1, latent_window_size], dim=1)\n",
    "            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)\n",
    "\n",
    "            clean_latents_4x, clean_latents_2x, clean_latents_1x = history_latents[:, :, -sum([16, 2, 1]):, :, :].split([16, 2, 1], dim=2)\n",
    "            clean_latents = torch.cat([start_latent.to(history_latents), clean_latents_1x], dim=2)\n",
    "\n",
    "            generated_latents = sample_hunyuan(\n",
    "                transformer=self.transformer,\n",
    "                sampler='unipc',\n",
    "                width=width,\n",
    "                height=height,\n",
    "                frames=latent_window_size * 4 - 3,\n",
    "                real_guidance_scale=cfg,\n",
    "                distilled_guidance_scale=gs,\n",
    "                guidance_rescale=rs,\n",
    "                num_inference_steps=steps,\n",
    "                generator=rnd,\n",
    "                prompt_embeds=llama_vec,\n",
    "                prompt_embeds_mask=llama_attention_mask,\n",
    "                prompt_poolers=clip_l_pooler,\n",
    "                negative_prompt_embeds=llama_vec_n,\n",
    "                negative_prompt_embeds_mask=llama_attention_mask_n,\n",
    "                negative_prompt_poolers=clip_l_pooler_n,\n",
    "                device=gpu,\n",
    "                dtype=torch.bfloat16,\n",
    "                image_embeddings=image_encoder_last_hidden_state,\n",
    "                latent_indices=latent_indices,\n",
    "                clean_latents=clean_latents,\n",
    "                clean_latent_indices=clean_latent_indices,\n",
    "                clean_latents_2x=clean_latents_2x,\n",
    "                clean_latent_2x_indices=clean_latent_2x_indices,\n",
    "                clean_latents_4x=clean_latents_4x,\n",
    "                clean_latent_4x_indices=clean_latent_4x_indices,\n",
    "                # Step-Adaptive CFG\n",
    "                adaptive_cfg_beta=adaptive_cfg_beta,\n",
    "                adaptive_cfg_min=1.0,\n",
    "            )\n",
    "\n",
    "            total_generated_latent_frames += int(generated_latents.shape[2])\n",
    "            history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)\n",
    "            \n",
    "            if not self.high_vram:\n",
    "                offload_model_from_device_for_memory_preservation(self.transformer, target_device=gpu, preserved_memory_gb=8)\n",
    "                load_model_as_complete(self.vae, target_device=gpu)\n",
    "\n",
    "            real_history_latents = history_latents[:, :, -total_generated_latent_frames:, :, :]\n",
    "            \n",
    "            if history_pixels is None:\n",
    "                history_pixels = vae_decode(real_history_latents, self.vae).cpu()\n",
    "            else:\n",
    "                 # Simplified stitching logic for benchmark purposes\n",
    "                section_latent_frames = latent_window_size * 2\n",
    "                overlapped_frames = latent_window_size * 4 - 3\n",
    "                current_pixels = vae_decode(real_history_latents[:, :, -section_latent_frames:], self.vae).cpu()\n",
    "                history_pixels = soft_append_bcthw(history_pixels, current_pixels, overlapped_frames)\n",
    "\n",
    "            if not self.high_vram:\n",
    "                unload_complete_models()\n",
    "                \n",
    "        return history_pixels\n",
    "\n",
    "def run_benchmark(prompts_file):\n",
    "    runner = BenchmarkRunner()\n",
    "    \n",
    "    # Check if inputs directory exists\n",
    "    inputs_dir = \"experiments/inputs\"\n",
    "    if not os.path.exists(inputs_dir):\n",
    "        print(f\"Warning: Inputs directory '{inputs_dir}' does not exist.\")\n",
    "    \n",
    "    with open(prompts_file, 'r') as f:\n",
    "        prompts = json.load(f)\n",
    "        \n",
    "    # Compare Baseline (beta=0.0) vs Proposed (beta=0.7)\n",
    "    betas = [0.0, 0.7]\n",
    "    \n",
    "    metadata_list = []\n",
    "    \n",
    "    for case in prompts:\n",
    "        prompt = case[\"prompt\"]\n",
    "        seed = case.get(\"seed\", 42)\n",
    "        category = case.get(\"category\", \"Unknown\")\n",
    "        case_id = case.get(\"id\", \"unk\")\n",
    "        \n",
    "        # Resolve Input Image Path\n",
    "        input_image_path = case.get(\"input_image\")\n",
    "        if not input_image_path or not os.path.exists(input_image_path):\n",
    "            print(f\"Error: Input image not found for case {case_id}: {input_image_path}\")\n",
    "            print(\"Skipping this case.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Loading input image: {input_image_path}\")\n",
    "        start_image = np.array(Image.open(input_image_path).convert(\"RGB\"))\n",
    "        \n",
    "        for beta in betas:\n",
    "            print(f\"\\n--- Processing {case_id} (Category: {category}) with Beta={beta} ---\")\n",
    "            \n",
    "            history_pixels = runner.generate(\n",
    "                prompt=prompt,\n",
    "                input_image=start_image,\n",
    "                seed=seed,\n",
    "                adaptive_cfg_beta=beta\n",
    "            )\n",
    "            \n",
    "            filename = f\"{case_id}_beta{beta}_{runner.timestamp}.mp4\"\n",
    "            filepath = os.path.join(runner.output_dir, filename)\n",
    "            \n",
    "            save_bcthw_as_mp4(history_pixels, filepath, fps=30, crf=16)\n",
    "            print(f\"Saved to {filepath}\")\n",
    "            \n",
    "            metadata_list.append({\n",
    "                \"filename\": filename,\n",
    "                \"prompt\": prompt,\n",
    "                \"category\": category,\n",
    "                \"beta\": beta,\n",
    "                \"case_id\": case_id,\n",
    "                \"seed\": seed,\n",
    "                \"input_image\": input_image_path\n",
    "            })\n",
    "            \n",
    "    # Save Metadata\n",
    "    metadata_path = os.path.join(runner.output_dir, \"metadata.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata_list, f, indent=2)\n",
    "    print(f\"Metadata saved to {metadata_path}\")\n",
    "    \n",
    "    # Run Evaluation\n",
    "    if len(metadata_list) > 0:\n",
    "        print(\"\\n--- Starting Evaluation ---\")\n",
    "        run_evaluation(\n",
    "            video_dir=runner.output_dir,\n",
    "            metadata_path=metadata_path,\n",
    "            output_dir=runner.output_dir, # Save eval results in the same folder\n",
    "            device='cuda'\n",
    "        )\n",
    "        print(\"\\n--- Benchmark Complete ---\")\n",
    "    else:\n",
    "        print(\"\\n--- No valid cases processed. Evaluation skipped. ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_benchmark(\"experiments/benchmark_prompts.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd9fa0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
      "Currently enabled native sdp backends: ['flash', 'math', 'mem_efficient', 'cudnn']\n",
      "Xformers is not installed!\n",
      "Flash Attn is not installed!\n",
      "Sage Attn is not installed!\n",
      "Warning: VBench not installed. Skipping VBench metrics.\n",
      "Loading models...\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "config.json: 100% 766/766 [00:00<00:00, 3.68MB/s]\n",
      "model.safetensors.index.json: 22.2kB [00:00, 47.8MB/s]\n",
      "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
      "Downloading (incomplete total...): 100% 14.9G/15.0G [01:53<00:00, 199MB/s]  \n",
      "Downloading (incomplete total...): 100% 15.0G/15.0G [01:53<00:00, 242MB/s]\n",
      "Fetching 4 files: 100% 4/4 [01:53<00:00, 28.36s/it] \u001b[A\n",
      "Download complete: 100% 15.0G/15.0G [01:53<00:00, 132MB/s]                \n",
      "Loading weights: 100% 290/290 [00:01<00:00, 240.52it/s, Materializing param=norm.weight]                              \n",
      "config.json: 100% 646/646 [00:00<00:00, 3.23MB/s]\n",
      "text_encoder_2/model.safetensors: 100% 246M/246M [00:03<00:00, 72.6MB/s] \n",
      "Loading weights: 100% 196/196 [00:00<00:00, 1323.73it/s, Materializing param=text_model.final_layer_norm.weight]                    \n",
      "tokenizer_config.json: 51.7kB [00:00, 102MB/s]\n",
      "tokenizer/tokenizer.json: 100% 17.2M/17.2M [00:00<00:00, 18.3MB/s]\n",
      "special_tokens_map.json: 100% 577/577 [00:00<00:00, 2.80MB/s]\n",
      "tokenizer_config.json: 100% 736/736 [00:00<00:00, 2.96MB/s]\n",
      "vocab.json: 1.06MB [00:00, 45.1MB/s]\n",
      "merges.txt: 525kB [00:00, 70.7MB/s]\n",
      "special_tokens_map.json: 100% 588/588 [00:00<00:00, 2.70MB/s]\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `hf_hub_download`. Downloading to a local directory does not use symlinks anymore.\n",
      "  warnings.warn(\n",
      "config.json: 100% 718/718 [00:00<00:00, 2.78MB/s]\n",
      "vae/diffusion_pytorch_model.safetensors: 100% 986M/986M [00:22<00:00, 43.6MB/s] \n",
      "preprocessor_config.json: 100% 394/394 [00:00<00:00, 1.10MB/s]\n",
      "config.json: 100% 585/585 [00:00<00:00, 2.62MB/s]\n",
      "image_encoder/model.safetensors: 100% 857M/857M [00:07<00:00, 118MB/s]  \n",
      "Loading weights: 100% 448/448 [00:00<00:00, 499.50it/s, Materializing param=vision_model.post_layernorm.weight]                      \n",
      "config.json: 100% 658/658 [00:00<00:00, 3.79MB/s]\n",
      "(…)ion_pytorch_model.safetensors.index.json: 134kB [00:00, 197MB/s]\n",
      "Downloading (incomplete total...): 0.00B [00:00, ?B/s]\n",
      "Downloading (incomplete total...):  99% 25.6G/25.7G [06:19<00:00, 207MB/s]   \n",
      "Downloading (incomplete total...): 100% 25.7G/25.7G [06:19<00:00, 204MB/s]\n",
      "Fetching 3 files: 100% 3/3 [06:19<00:00, 126.66s/it]\u001b[A\n",
      "Download complete: 100% 25.7G/25.7G [06:19<00:00, 67.8MB/s]               \n",
      "Loading checkpoint shards: 100% 3/3 [00:02<00:00,  1.45it/s]\n",
      "Models loaded successfully.\n",
      "Loading input image: experiments/inputs/434605182-f3bc35cf-656a-4c9c-a83a-bbab24858b09.jpg\n",
      "\n",
      "--- Processing A1 (Category: A_Standard) with Beta=0.0 ---\n",
      "Generating for prompt: 'A young woman with a gentle smile, standing in a sunlit garden, looking directly at the camera. High quality, 4k.' with beta=0.0, seed=42\n",
      "Unloaded LlamaModel as complete.\n",
      "Unloaded CLIPTextModel as complete.\n",
      "Unloaded SiglipVisionModel as complete.\n",
      "Unloaded AutoencoderKLHunyuanVideo as complete.\n",
      "Unloaded DynamicSwap_HunyuanVideoTransformer3DModelPacked as complete.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/WorldModel_Group04/experiments/run_benchmark.py\", line 304, in <module>\n",
      "    run_benchmark(\"experiments/benchmark_prompts.json\")\n",
      "  File \"/content/WorldModel_Group04/experiments/run_benchmark.py\", line 261, in run_benchmark\n",
      "    history_pixels = runner.generate(\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/WorldModel_Group04/experiments/run_benchmark.py\", line 99, in generate\n",
      "    load_model_as_complete(self.text_encoder, target_device=gpu)\n",
      "  File \"/content/WorldModel_Group04/diffusers_helper/memory.py\", line 130, in load_model_as_complete\n",
      "    model.to(device=target_device)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\", line 3587, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1371, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 930, in _apply\n",
      "    module._apply(fn)\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 957, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1357, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# 実行\n",
    "!python experiments/run_benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad6beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
