%%\documentstyle[twocolumn,jsaiac]{jarticle}
%%\documentstyle[twocolumn,jsaiac]{j-article}
\documentclass[twocolumn]{jarticle}

\usepackage{jsaiac}

\usepackage{color}

%%
\title{
\jtitle{20XX年度人工知能学会全国大会・\LaTeX{}スタイルファイル}
\etitle{\LaTeX{} Style file for manuscripts of JSAI 20XX}
}
%%英文は以下を使用
%\title{Style file for manuscripts of JSAI 20XX}

\jaddress{氏名，所属，住所，電話番号，Fax番号，電子メールアドレスなど}

\author{%
\jname{第XX回全国大会プログラム委員会\first}
\ename{The Program Committee of the XXth annual conference of JSAI}
\and
\jname{第2筆者氏名\second}
\ename{Second Author's Name}
%\and
%Given-name Surname\third{}%%英文は左を使用
}

\affiliate{
\jname{\first{}人工知能学会}
\ename{The Japanese Society for Artificial Intelligence}
\and
\jname{\second{}所属和文2}
\ename{Affiliation \#2 in English}
%\and
%\third{}Affiliation \#3 in English%%英文は左を使用
}

%%
%\Vol{28}        %% <-- 28th（変更しないでください）
%\session{0A0-00}%% <-- 講演ID（必須)

\begin{abstract}
Here is an abstract of up to 150 words in English or 300 characters in Japanese. 
This document describes the format guidelines for Japanese manuscripts in \LaTeX{} of the annual conference of JSAI. 
This is also a sample of a formatted manuscripts (see 2.4 for writing the abstract).
\end{abstract}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\maketitle
\section{研究背景}
動画生成AI、特にDiffusion Transformer（DiT）ベースのモデルにおいて、生成時間の「長さ」と「品質」の両立は最大の技術的障壁の一つである。数秒の動画生成であれば、全フレームを一度にメモリに展開し、相互の注意機構（Attention Mechanism）によって一貫性を保つことが可能である。しかし、分単位あるいは無限の長尺動画を生成する場合、メモリ制約から「スライディングウィンドウ」や「自己回帰的（Autoregressive）」な手法を取らざるを得ない。
ここで発生するのが「忘却（Forgetting）」と「ドリフト（Drifting）」という二つの相反する問題である。
忘却とは過去の文脈が失われ、時間がたつにつれて登場人物の服の色や背景といったディテールが変質する現象のことであり、ドリフトとは過去の文脈を重視しすぎたり生成された微細なエラーが蓄積することで、映像の崩壊や動きの消失等が発生する現象である。この二つの問題はトレードオフの関係になっており、忘却に対応するため記憶能力をあげると、細かいエラーが蓄積しやすくなりドリフトを促進するが、ドリフトに対処すると誤差が伝搬しにくくなるものの、同時にディテールの情報も失われ、忘却が発生しやすくなる。
また記憶能力の向上には計算量の増大がつきものであり、トランスフォーマーにおいてエンコードするフレーム数を増やすと計算量を$O(n^2)$で増大させてしまう。%ここはいらないかもしれない
FramePack は、このジレンマに対し「階層的メモリ（Hierarchical Memory）」という解を提示した。これは、直近のフレームを高解像度で保持しつつ、過去のフレームを幾何級数的に圧縮して保持することで、理論上無限のコンテキストを固定長のトークン数で扱う技術である。
しかし、ユーザーの実体験として報告されている「プロンプトによる『消失』指示の無視」は、この「記憶の保持」メカニズムが過剰に作用している可能性を示唆している。ユーザーが「被写体が消える」と指示しても、モデルが保持する「過去の記憶（被写体が存在する状態）」が強力な制約として働き、プロンプトを無視して被写体を再描画し続けてしまうのである。
\if0

ここに研究内容を書く

\fi

\section{関連研究}



\begin{thebibliography}{99}
\bibitem[Knuth 84]{texbook}
 Knuth,~D.~E.: The \TeX{}book, Addison-Wesley (1984),
  (邦訳~: \TeX{}ブック, 斎藤 信男 監修, 鷺谷 好輝 訳,
  アスキー出版局 (1992)).
\bibitem[Lamport 86]{latexブック}
Leslie,~L: \LaTeX{}: {A} Document Preparation System (Updated for
  \LaTeX{}2$\varepsilon$), Addison-Wesley, 2nd edition (1998)
  (邦訳~: 文書処理システム \LaTeX{}2$\varepsilon$,
  阿瀬 はる美 訳, ピアソン・エデュケーション, (1999)).
\end{thebibliography}
%%

\end{document}
