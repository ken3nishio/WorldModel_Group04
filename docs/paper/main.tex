\documentclass[twocolumn]{ujarticle}

\usepackage{jsai}
\usepackage[dvipdfmx]{graphicx}
\usepackage{url}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{booktabs}
\usepackage[dvipdfmx]{color}

%%
\title{
\jtitle{大規模動画生成モデルにおける「時間的慣性」の適応的制御：\\Adaptive CFGとTemporal Unlearningによる動的状態遷移の実現}
\etitle{Adaptive Control of Temporal Inertia in Large-Scale Video Generation Models}
}

\jaddress{*1 sr24584e@st.omu.ac.jp\first, *2 nishio@ou.ac.jp\second, *3 junko.ami@csis.u-tokyo.ac.jp\third}

\author{%
\jname{新川大翔\first, 西尾 謙一\second, 網 淳子\third }
\ename{Hiroto Shinkawa, Kennichi Nishio, Junko Ami}
}

\affiliate{
\jname{\first{大阪公立大学理学部}. \second{大阪大学工学部} \third{東京大学大学院工学系研究科先端学際工学専攻}}
\ename{Osaka Metropolitan University, Osaka University, The University of Tokyo}
}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\begin{abstract}
HunyuanVideoやFramePackなどの近年の大規模動画生成モデルは、極めて高い時間的一貫性（Temporal Consistency）を実現している。しかし、この特性は「物体が消失する」あるいは「静止から急激に跳躍する（バク転）」といった非連続的な状態遷移において、初期状態を過剰に維持しようとする \textbf{"Static Death"（静的死）} と呼ばれる現象を引き起こす。本研究では、この課題の本質がモデルの持つ過剰な「時間的慣性」にあると定義し、推論時の動的介入によってこれを制御する手法を提案する。具体的には、(1) 生成初期に過剰な拘束を緩和する \textbf{Adaptive CFG (Relaxation)}、(2) その緩和を適切に減衰させ構造崩壊を防ぐ \textbf{Decay Power ($p$)}、(3) 過去のフレームへの固執を緩和する \textbf{Temporal Blur (Gaussian Blur)} の3要素を導入する。実験の結果、これらのパラメータ制御により、従来モデルでは不可能であった非連続的な状態遷移を伴う動的アクションの生成を、映像の破綻を防ぎつつ実現できることを定量的に実証した。
\end{abstract}

\maketitle

\section{はじめに}
動画生成AI、特にDiffusion Transformer (DiT) ベースのモデル \cite{hunyuan} は、近年目覚ましい進化を遂げている。数秒の動画生成であれば、全フレームを一度にメモリに展開し、相互の注意機構 (Self-Attention) によって高い一貫性を保つことが可能である。しかし、分単位あるいは無限の長尺動画を生成する場合、メモリ制約から「スライディングウィンドウ」や「自己回帰的 (Autoregressive)」な手法を取らざるを得ない。

FramePack 等の自己回帰モデルは、過去のフレームを参照して現在のフレームを生成することで、長時間の整合性を維持する。しかし、この強力な「過去への参照」は諸刃の剣でもある。我々の予備実験において、これらのモデルは「消える (disappear)」「バク転する (backflip)」といった、非連続的な状態遷移を伴うプロンプトを無視し、初期状態（立ち姿など）を維持し続ける傾向が確認された。我々はこの現象を、モデルが過去の文脈に過剰に囚われる \textbf{"Static Death"（静的死）} と呼ぶ。

本研究の目的は、この "Static Death" を克服し、学習済みモデルのパラメータを変更することなく（Training-free）、推論時の介入のみで動的なアクション生成を実現することである。

\section{関連研究}

\subsection{大規模動画生成モデルと一貫性}
Sora や HunyuanVideo \cite{hunyuan} に代表される大規模モデルは、膨大なデータセットによる学習を通じて、物理法則や動作の事前知識を獲得している。しかし、生成時においては「時間的一貫性 (Temporal Consistency)」が最優先される傾向にあり、特に自己回帰的な生成においては、直前のフレームとの連続性が強く強制される。これは、背景の固定や人物のアイデンティティ維持には有効だが、急激なアクションの生成を阻害する要因となる。

\subsection{推論時介入による制御}
学習済みモデルに追加学習を行わずに制御を行う試みとして、FreeInit \cite{freeinit} や Adaptive Low-Pass Guidance (ALG) \cite{alg} がある。FreeInit は初期ノイズの再利用によって一貫性を向上させる手法であり、逆に言えば Static Death を助長する可能性がある。一方、ALG は条件画像から高周波成分を除去することで動きを促進する手法であり、本研究の方向性と近い。しかし、ALG はあくまで入力画像の前処理に留まっており、拡散モデルの生成ダイナミクスそのもの（CFG など）への介入は行なっていない。

\subsection{拡散モデルの周波数特性と構造形成}
拡散モデルの生成プロセスには、周波数領域における明確なバイアスが存在することが知られている。Choiら \cite{freeinit} の分析によれば、デノイジングプロセスの初期段階（高ノイズ領域）では、画像の「低周波成分（大まかな構図・配置）」が決定され、終盤（低ノイズ領域）にかけて「高周波成分（細部・テクスチャ）」が形成される。
既存の FreeInit 等の手法は、この特性を利用し、初期ステップの潜在変数を固定・反復することで、低周波成分（構造）の一貫性を強化しようとした。これは「歩行」のような定常的な動作には有効であるが、「バク転」のようなグローバルな構造変化（直立 $\to$ 倒立）を伴うアクションにおいては、致命的な制約となる。既存の構造バイアスが初期段階で固定されてしまうため、モデルは新たな姿勢へと遷移できなくなるのである。
対して本研究のアプローチは、この特性を逆手に取るものである。我々は、デノイジングプロセスの初期段階（低周波成分生成フェーズ）においてのみ、CFGやAttentionによる拘束を意図的に破壊（Relaxation）することで、非連続な状態遷移を許容する。提案手法における $\beta$ と $p$ は、まさにこの低周波成分生成フェーズへの介入強度と期間を制御するパラメータと解釈できる。これにより、モデルは学習済みの物理知識（どう回転すべきか）を維持しつつ、初期フレーム（立ち姿）の構造的制約から解放されることが可能となる。    

\section{課題分析と仮説：なぜStatic Deathは起きるのか}

\subsection{エネルギー地形による解釈}
拡散モデルの生成過程は、エネルギーポテンシャルの斜面を下り、安定状態（極小値）へ収束するプロセスと見なせる。Static Death が発生している状態では、初期フレーム（立ち姿）の周辺に、極めて深く急峻なポテンシャルの谷（Deep Valley）が形成されていると考えられる。モデルはこの谷底に捕らわれており、プロンプトが「バク転」を指示しても、そのエネルギー障壁を越えて別の谷（バク転状態）へ遷移することができない。

\subsection{初期仮説の失敗：Impulse (Boost) アプローチ}
当初、我々はこの障壁を越えるためには「強い力」が必要であると考えた。すなわち、CFGスケールを負の値に設定したり（$\beta < 0$）、初期ノイズを増幅させることで、モデルを強制的に谷から押し出す \textbf{"Impulse" (Boost)} 戦略である。
実際に $\beta = -1.0$ （初期CFGを強化）として予備実験を行ったところ、VideoMAEスコアは $0.007$ と Baseline ($0.004$) に比して有意な改善が見られず、Top-1 予測クラスも "spinning poi" ($7\%$) や "breakdancing" という脈絡のない結果となった。
映像を詳細に分析すると、人物がバク転をするのではなく、手足が異常に伸長したり、多重露光のように身体が分裂したりする「構造的崩壊（Morphological Collapse）」が確認された。これは、過剰なエネルギー注入により、モデルがポテンシャルの谷を越えるどころか、地形そのものを破壊し、物理的に成立しないカオス状態（Spinning Poiのような意味不明な回転）へと発散してしまったことを意味する。
この結果から、単に「力を加える」だけでは、繊細な構造変化（バク転）を誘導することは不可能であり、逆に「構造を壊してしまう」リスクが高いことが判明した。

\subsection{修正仮説：Relaxation アプローチ}
この失敗から、我々は「力で押すのではなく、壁を低くすればよいのではないか」という着想を得た。すなわち、生成の初期段階においてのみ、CFGスケールやAttentionの制約を意図的に弱めることで、ポテンシャル地形全体を平坦化（Flattening）する \textbf{"Relaxation"} 戦略である。
障壁が低くなれば、モデルはわずかな駆動力（プロンプトの指示）でも容易に現在の谷を脱出し、隣接する目的の谷へと遷移できるはずである。本研究ではこの仮説に基づき、適応的なパラメータ制御手法を設計した。

\section{提案手法}

本研究では、"Static Death" を克服するために、拡散モデルの推論プロセスに介入する2つの手法、\textbf{Adaptive CFG (Relaxation)} と \textbf{Temporal Unlearning} を提案する。これらの手法は、先行研究で明らかにされた拡散モデルの特性に基づき設計されている。

\subsection{Adaptive CFGによる初期緩和}
通常のclassifier-free guidance (CFG) におけるノイズ予測 $\hat{\epsilon}_t$ は、条件付き予測 $\epsilon_\theta(x_t, c)$ と無条件予測 $\epsilon_\theta(x_t, \emptyset)$ を用いて以下の式で表される：
\begin{equation}
\hat{\epsilon}_t = \epsilon_\theta(x_t, \emptyset) + s \cdot (\epsilon_\theta(x_t, c) - \epsilon_\theta(x_t, \emptyset))
\end{equation}
ここで $s$ は定数のガイダンススケールである。

\subsubsection{Relaxation Strength $\beta$: ALGからの拡張}
先行研究であるALG \cite{alg} は、初期入力の高周波成分を落とすことで動きを促進できることを示した。これは構造的制約を弱めることを意味する。我々はこの知見を拡張し、入力画像だけでなく、生成プロセスそのものの制約（CFGスケール）を初期段階で弱めることで、より直接的にモデルの探索空間を広げることができると考えた。
具体的には、初期のガイダンススケールを減衰させる係数 $\beta$ を導入する。これにより、モデルはプロンプトの指示に対してより敏感に反応できるようになる。

\subsubsection{Decay Power $p$: FreeInitの周波数特性に基づく制御}
FreeInit \cite{freeinit} は、拡散プロセスの初期段階が画像の「低周波成分（大まかな構図・配置）」を決定的に左右することを示した。逆に言えば、初期段階を過ぎれば構図・配置は固定され、以降はディテールの生成に移行する。
したがって、Relaxation（構造破壊）の効果は、構図・配置が決定される初期段階のみに限定されるべきであり、後半まで持続するとディテールの崩壊を招く。この介入の引き際を厳密に制御するために、減衰パラメータ $p$ を導入した。
提案する \textbf{Relaxation Curve} は以下の通りである：
\begin{equation}
s(\sigma_t) = s_{min} + (s_{base} - s_{min}) \cdot (1 - \beta \cdot \sigma_t^p)
\end{equation}
各パラメータの定義と役割は以下の通りである。
\begin{itemize}
    \item $s_{base}$: ターゲットとする基本CFGスケール（例: 6.0）。
    \item $s_{min}$: 緩和時の最小CFGスケール（例: 1.0）。
    \item $\beta \in [0, 1]$ (\textbf{Relaxation Strength}): 初期ガイダンスの減衰率。ALGの知見に基づき、初期の構造的制約を緩和する度合いを決定する。$\beta=1.0$ のとき、初期スケールは $s_{min}$ まで低下し、探索自由度を最大化する。
    \item $p$ (\textbf{Decay Power}): FreeInitの知見に基づき、低周波生成フェーズ（初期）から高周波生成フェーズ（後半）への移行に合わせて緩和を終了させるための減衰指数。$p > 1$（例: 2.0）とすることで、初期の緩和状態から急速に $s_{base}$ へ復帰させ、後半の整合性を担保する。
\end{itemize}

\subsection{Temporal Blur: Attentionマップの拡散}
Prompt-to-Prompt \cite{p2p} は、生成される物体の形状や位置（アイデンティティ）が Self-Attention マップに強く依存していることを明らかにした。Static Death が起きている状態では、過去フレームの Attention マップ（立ち姿）が強固に維持されていると解釈できる。
そこで本手法では、Self-Attention 層の Key ($K$) および Value ($V$) に対して時間軸方向の平滑化（Gaussian Blur）を適用する \textbf{Temporal Blur} を導入した。
\begin{equation}
\tilde{K}_{\tau} = \sum_{j} G(j - \tau; \sigma_{blur}) \cdot K_{j}
\end{equation}
これにより、Attention における時間的な局所性（過去への固執）を物理的に拡散させ、プロンプトによる新しい状態への遷移（Overwrite）を容易にする。これは ALG のような入力画像処理ではなく、モデル内部の記憶機構への直接介入である。

\input{experiment_results.tex}

\section{結論}
本研究では、大規模動画生成モデルにおける深刻な課題 "Static Death" を定義し、その克服に向けた推論時介入手法を提案した。
本研究の主な貢献は以下の3点に集約される。
\begin{enumerate}
    \item \textbf{Static Deathの現象論的解明}: モデルが過去の文脈に過剰に囚われる現象を「エネルギー地形における深い局所解への埋没」としてモデル化し、従来のインパルス（Boost）アプローチがカオスを招くことを明らかにした。
    \item \textbf{Relaxation理論の確立}: 力で脱出するのではなく、初期拘束を一時的に緩和して障壁を下げる「Relaxation」アプローチを提唱し、その有効性を理論的・実験的に実証した。
    \item \textbf{劇的な性能向上}: 提案手法により、追加学習なしでVideoMAEスコアを約5.4倍（0.073 $\rightarrow$ 0.394）に向上させ、従来不可能であった「バク転」等の動的アクション生成を実現した。
\end{enumerate}
本手法は、高画質化・安定化が進む今後の動画生成モデルにおいて、失われがちな「動的な表現力」を取り戻すための標準的な制御技術となることが期待される。

\begin{thebibliography}{99}
\bibitem{hunyuan} HunyuanVideo Authors, et al. "HunyuanVideo: A Large-scale Video Generation Model." 2024.
\bibitem{freeinit} Wu, T., et al. "FreeInit: Bridging Initialization and Inference for better Video Generation." 2023.
\bibitem{p2p} Hertz, A., et al. "Prompt-to-Prompt Image Editing with Cross Attention Control." 2022.
\bibitem{alg} Gu, Y., et al. "Adaptive Low-Pass Guidance for Image-to-Video Generation." 2024.
\bibitem{svi} Research Team, "Stable Video Infinity." 2024.
\bibitem{motionrag} Research Team, "MotionRAG: Retrieval-Augmented Motion Generation." 2024.
\end{thebibliography}
%%

\end{document}
