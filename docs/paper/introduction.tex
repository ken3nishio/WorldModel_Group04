%%\documentstyle[twocolumn,jsaiac]{jarticle}
%%\documentstyle[twocolumn,jsaiac]{j-article}
\documentclass[twocolumn]{jarticle}

\usepackage{jsaiac}

\usepackage{color}

%%
\title{
\jtitle{2026年度人工知能学会全国大会・\LaTeX{}スタイルファイル}
\etitle{\LaTeX{} Style file for manuscripts of JSAI 20XX}
}
%%英文は以下を使用
%\title{Style file for manuscripts of JSAI 20XX}

\jaddress{新川大翔，大阪公立大学}

\author{%
\jname{新川大翔\first}
\ename{Hiroto Shinkawa}
\and
\jname{西尾 謙一\second}
\ename{ Nishio}
%\and
%Given-name Surname\third{}%%英文は左を使用
}

\affiliate{
\jname{\first{}大阪公立大学}
\ename{Osaka Metropolitan University}
\and
\jname{\second{}大阪大学}
\ename{Osaka University}
%\and
%\third{}Affiliation \#3 in English%%英文は左を使用
}

%%
%\Vol{28}        %% <-- 28th(変更しないでください)
%\session{0A0-00}%% <-- 講演ID(必須)

\begin{abstract}
Here is an abstract of up to 150 words in English or 300 characters in Japanese. 
This document describes the format guidelines for Japanese manuscripts in \LaTeX{} of the annual conference of JSAI. 
This is also a sample of a formatted manuscripts (see 2.4 for writing the abstract).
\end{abstract}

%\setcounter{page}{1}
\def\Style{``jsaiac.sty''}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em%
 T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\def\JBibTeX{\leavevmode\lower .6ex\hbox{J}\kern-0.15em\BibTeX}
\def\LaTeXe{\LaTeX\kern.15em2$_{\textstyle\varepsilon}$}

\begin{document}
\maketitle
\section{研究背景}
動画生成AI、特にDiffusion Transformer(DiT)ベースのモデルにおいて、生成時間の「長さ」と「品質」の両立は最大の技術的障壁の一つである。数秒の動画生成であれば、全フレームを一度にメモリに展開し、相互の注意機構(Attention Mechanism)によって一貫性を保つことが可能である。しかし、分単位あるいは無限の長尺動画を生成する場合、メモリ制約から「スライディングウィンドウ」や「自己回帰的(Autoregressive)」な手法を取らざるを得ない。

ここで発生するのが「忘却(Forgetting)」と「ドリフト(Drifting)」という二つの相反する問題である。
忘却とは過去の文脈が失われ、時間がたつにつれて登場人物の服の色や背景のオブジェクトといったディテールが変質する現象のことであり、ドリフトとは過去の文脈を重視しすぎたり生成された微細なエラーが蓄積することで、映像の崩壊や動きの消失等が発生する現象である。この二つの問題はトレードオフの関係になっており、忘却に対応するため記憶能力をあげると、細かいエラーが蓄積しやすくなりドリフトを促進するが、ドリフトに対処すると誤差が伝搬しにくくなるものの、同時にディテールの情報も失われ、忘却が発生しやすくなる。
また記憶能力の向上には計算量の増大がつきものであり、トランスフォーマーにおいてエンコードするフレーム数を増やすと計算量を$O(n^2)$で増大させてしまう。%ここはいらないかもしれない
FramePack は、このジレンマに対し「階層的メモリ(Hierarchical Memory)」という解を提示した。これは、直近のフレームを高解像度で保持しつつ、過去のフレームを幾何級数的に圧縮して保持することで、理論上無限のコンテキストを固定長のトークン数で扱う技術である。

しかし、我々の予備実験においてFramePackが過去のコンテキストと不連続な変化を支持するプロンプトを無視することが確認された。具体的には、歩いている男性の写真に対して「消える(disappear)」、「画面外に出ていく(go outside of the frame)」等のプロンプトを入力すると、歩き続ける男性の動画が生成されてしまった。

ユーザーの実体験として報告されている「プロンプトによる『消失』指示の無視」は、この「記憶の保持」メカニズムが過剰に作用している可能性を示唆している。ユーザーが「被写体が消える」と指示しても、モデルが保持する「過去の記憶(被写体が存在する状態)」が強力な制約として働き、プロンプトを無視して被写体を再描画し続けてしまうのである。
\if0

ここに研究内容を書く

\fi

\section{関連研究}

\subsection{周波数領域からのアプローチ(Adaptive Low-Pass Guidance)}
Adaptive Low-Pass Guidance(ALG)はALGは、Image-to-Videoモデルにおいて「動きのダイナミクスが抑制される」原因を、周波数領域の観点から分析した研究である。ALGはデノイジングプロセスの初期段階(タイムステップ $T$ が大きい時)において、条件画像(Image Condition)の高周波成分がモデルに「過剰な構造的制約」を与えていることに着目し、推論時(Inference-time)において、デノイジングの初期ステップでは条件画像にガウシアンブラーなどのローパスフィルタ(Low-Pass Filter)を適用し、高周波成分を除去した状態でモデルに入力した。具体的には、初期ステップにおいてはブラーを強めて抽象度をあげることでトランジションによる状態の変化量を減らし、後期ステップにおいてはフィルタを徐々に弱めて元の条件画像を入力することでディテール等を元画像と整合性を保つ、というものである。

このアプローチは、「積極的に情報を捨てる(ボヤけさせる)」ことが、逆にダイナミックな変化を生む鍵であることを示唆しており、また実装コストも低く再学習も不要なため、本研究における\textbf{Step-Adaptive CFG}の着想元となった。

\subsection{ストリーミング生成と同時デノイジング(Rolling Forcing / SVI)}
FramePackのような自己回帰(Autoregressive)モデルの弱点は、1フレームずつ(あるいはチャンクずつ)確定させていくプロセスにおいて、前のフレームのエラーが次のフレームに波及し、モデルが保守的になる点にある。これに対し、Rolling Forcing および SVI (Stable Video Infinity) はストリーミング生成の枠組みで「変化」を許容することを可能とする。
Rolling Forcingは、フレーム$F_N$を生成する際に$F_N~F_{N+k}$を異なるノイズレベルで同時にデノイズする。通常の自己回帰ではフレーム $F_N$ を生成したのちに$F_{N+1}$を生成するため$F_N$と$F_{N+1}$の間には厳密な因果律が存在する。一方Rolling Forcingは$F_N~F_{N+k}$をまとめてデノイズするためフレーム間の因果律が弱まり、フレーム間での急激な変化(消失など)もノイズの中で徐々に準備させることができるようになる。

また、SVIは学習時にモデル自身が生成した「エラーを含むフレーム」をあえて次の学習ステップの入力として使用することで、「エラーから回復する」、あるいは「エラーを含んだ状態からでも正しく変化を続ける」能力を学習させる。この学習法は、学習時にはきれいなデータを入力とするが推論時には自身の生成した「エラーを含むフレーム」を使わなければならない、という「学習時と推論時のギャップ(Exposure Bias)」を埋めることができ、モデルは多少の破綻や不連続性を恐れなくなり、プロンプトの指示に従って大胆な変化(Drastic Change)を起こす際のリスク(アーティファクトの発生)を許容できるようになる。

しかしながら，Rolling ForcingおよびSVIはいずれも学習手法の変更とモデルの再学習を必要とし，大規模な計算資源と実装上の工数を要するため，既存の事前学習モデルに対して容易に適用できないという課題がある。

\subsection{検索拡張による動作転送(MotionRAG)}
MotionRAGは、RAG(Retrieval-Augmented Generation)の概念を動画生成に応用したものである。ユーザーが「馬に乗る人」というプロンプトを入力した際、モデルはまず外部のビデオデータベースから、意味的に類似した(つまり、何かが消えている)ビデオクリップを検索・取得する。この際に単に別の動画の動きをコピーするだけでは、被写体の形状や背景が異なるため破綻すため、MotionRAGは「Context-Aware Motion Adaptation (CAMA)」モジュールを用いて検索された動画の「動きの抽象的な特徴(Motion Prior)」のみを抽出し、それをターゲット画像の文脈に合わせて適応させることで例えば「馬に乗る人」の動きを「バイクに乗る人」の動きに適用することができるようになる。

この手法を用いるとモデルの内部パラメータに依存せず強制的に動作を注入することで「消失」や「トランジション」等を生成できる可能性があるが、FramePackに組み込む上で実装コストが高く、モデル自体も再学習が必要な可能性があるため適用させるには向かないと判断した。



\begin{thebibliography}{99}
\bibitem[Knuth 84]{texbook}
 Knuth,~D.~E.: The \TeX{}book, Addison-Wesley (1984),
  (邦訳~: \TeX{}ブック, 斎藤 信男 監修, 鷺谷 好輝 訳,
  アスキー出版局 (1992)).
\bibitem[Lamport 86]{latexブック}
Leslie,~L: \LaTeX{}: {A} Document Preparation System (Updated for
  \LaTeX{}2$\varepsilon$), Addison-Wesley, 2nd edition (1998)
  (邦訳~: 文書処理システム \LaTeX{}2$\varepsilon$,
  阿瀬 はる美 訳, ピアソン・エデュケーション, (1999)).
\end{thebibliography}
%%

\end{document}